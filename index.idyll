[meta title:"DeepEq" description:"An educational platform to help students develop a mathematical intuition for deep learning" /]

[FullWidth]
[flowChart tutorials:`["1. Perceptron", "2. Loss Function", "3. Gradient Descent", "4. Backpropagation Theory", "5. Backpropagation Implementation"]`]

[Step]

## Perceptron

A biological brain consists of many interconnected neurons, which is what scientists tried to replicate.
As a result, they came up with the notion of an artificial neuron called a perceptron.
The perceptron is the foundation of deep learning and is made up of several inputs [equation] x_i [/equation].
Some inputs are more important than others which is why they are multiplied with parameters called weights [equation] w_i [/equation].
For example, if we want to train a computer to distinguish between a dog and a spider, the number of legs would be more important (higher weight) than the color.
Finally, everything is summed up and an activation function [equation] a(x) [/equation] is applied.
An interactive perceptron with three inputs and activation function [equation] tanh(x) [/equation] can be found below.

[var name:"perceptronX1" value:1 /]
[var name:"perceptronX2" value:2 /]
[var name:"perceptronX3" value:3 /]
[var name:"perceptronW1" value:1 /]
[var name:"perceptronW2" value:1 /]
[var name:"perceptronW3" value:1 /]
[derived name:"perceptronX4" value:`Number(perceptronX1*perceptronW1+perceptronX2*perceptronW2+perceptronX3*perceptronW3).toFixed(1)` /]
[derived name:"perceptronY" value:`Math.tanh(Number(perceptronX1*perceptronW1+perceptronX2*perceptronW2+perceptronX3*perceptronW3)).toFixed(1)` /]
[SVG viewBox:"0 0 1000 500"]
  [ellipse fill:"#fff" stroke:"#000" strokeWidth:"1.5" cx:"150" cy:"100" rx:"45" ry:"45"/]
  [ellipse fill:"#fff" stroke:"#000" strokeWidth:"1.5" cx:"150" cy:"250.00001" rx:"45" ry:"45"/]
  [ellipse fill:"#fff" stroke:"#000" strokeWidth:"1.5" cx:"150" cy:"400" rx:"45" ry:"45"/]
  [ellipse fill:"#fff" stroke:"#000" strokeWidth:"1.5" cx:"500" cy:"250.00001" rx:"45" ry:"45"/]
  [ellipse fill:"#fff" stroke:"#000" strokeWidth:"1.5" cx:"849.23077" cy:"250.00001" rx:"45" ry:"45"/]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" strokeOpacity:"null" fillOpacity:"null" x:"45.49999" y:"410.50002" fontSize:"50" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" ]x[/text]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" strokeOpacity:"null" fillOpacity:"null" x:"70.49999" y:"423.50002" fontSize:"32" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" ]3[/text]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" strokeOpacity:"null" fillOpacity:"null" x:"45.49999" y:"265.50001" fontSize:"50" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" ]x[/text]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" strokeOpacity:"null" fillOpacity:"null" x:"70.49999" y:"278.50001" fontSize:"32" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" ]2[/text]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" strokeOpacity:"null" fillOpacity:"null" x:"45.49999" y:"116.49999" fontSize:"50" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" ]x[/text]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" strokeOpacity:"null" fillOpacity:"null" x:"70.49999" y:"129.49999" fontSize:"32" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" ]1[/text]
  [SvgPath d:"m261.5,49.5" opacity:"0.5" fillOpacity:"null" strokeOpacity:"null" strokeWidth:"1.5" stroke:"#000" fill:"#fff"/]
  [SvgPath stroke:"black" d:"m194.5,99.58592c296.28027,-2.82856 288.33124,16.77148 303,104.91408" opacity:"0.5" fillOpacity:"null" strokeOpacity:"null" strokeWidth:"1.5" fill:"#fff"/]
  [SvgPath stroke:"black" d:"m194.5,247.5l261.5,-1.5" opacity:"0.5" fillOpacity:"null" strokeOpacity:"null" strokeWidth:"1.5" fill:"#fff"/]
  [SvgPath stroke:"black" d:"m194.5,398.07526c296.28027,2.79245 288.33124,-16.55746 303,-103.57526" opacity:"0.5" fillOpacity:"null" strokeOpacity:"null" strokeWidth:"1.5" fill:"#fff"/]
  [SvgPath stroke:"#000" d:"m544.85602,250.51953l259.19238,-1.5" opacity:"0.5" fillOpacity:"null" strokeOpacity:"null" strokeWidth:"1.5" fill:"#fff"/]
  [SvgText value:perceptronX1 textAnchor:"start" fontFamily:"Helvetica, Arial, sans-serif" fontSize:"47" y:"115.76917" x:"138.07674" strokeOpacity:"null" strokeWidth:"0" stroke:"#000" fill:"#000000" /]
  [SvgText value:perceptronX2 textAnchor:"start" fontFamily:"Helvetica, Arial, sans-serif" fontSize:"47" y:"265.76926" x:"137.30751" strokeOpacity:"null" strokeWidth:"0" stroke:"#000" fill:"#000000" /]
  [SvgText value:perceptronX3 textAnchor:"start" fontFamily:"Helvetica, Arial, sans-serif" fontSize:"47" y:"415.00011" x:"138.07674" strokeOpacity:"null" strokeWidth:"0" stroke:"#000" fill:"#000000" /]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" strokeOpacity:"null" fillOpacity:"null" x:"238.12029" y:"79.57689" fontSize:"50" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" ]w[/text]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" strokeOpacity:"null" fillOpacity:"null" x:"269.50491" y:"92.57689" fontSize:"32" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" ]1[/text]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" strokeOpacity:"null" fillOpacity:"null" x:"238.11549" y:"225.11544" fontSize:"50" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" ]w[/text]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" strokeOpacity:"null" fillOpacity:"null" x:"269.50011" y:"238.11544" fontSize:"32" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" ]2[/text]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" strokeOpacity:"null" fillOpacity:"null" x:"238.11549" y:"376.11544" fontSize:"50" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" ]w[/text]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" strokeOpacity:"null" fillOpacity:"null" x:"269.50011" y:"389.11544" fontSize:"32" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" ]3[/text]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" strokeOpacity:"null" fillOpacity:"null" x:"288.49999" y:"80.49999" fontSize:"50" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" ]:[/text]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" strokeOpacity:"null" fillOpacity:"null" x:"288.49999" y:"226.49999" fontSize:"50" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" ]:[/text]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" strokeOpacity:"null" fillOpacity:"null" x:"288.49999" y:"375.49999" fontSize:"50" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" ]:[/text]
  [SvgText value:perceptronW1 textAnchor:"start" fontFamily:"Helvetica, Arial, sans-serif" fontSize:"47" y:"79.76917" x:"331.07674" strokeOpacity:"null" strokeWidth:"0" stroke:"#000" fill:"#000000" /]
  [SvgText value:perceptronW2 textAnchor:"start" fontFamily:"Helvetica, Arial, sans-serif" fontSize:"47" y:"226.76917" x:"331.07674" strokeOpacity:"null" strokeWidth:"0" stroke:"#000" fill:"#000000" /]
  [SvgText value:perceptronW3 textAnchor:"start" fontFamily:"Helvetica, Arial, sans-serif" fontSize:"47" y:"375.76917" x:"331.07674" strokeOpacity:"null" strokeWidth:"0" stroke:"#000" fill:"#000000" /]
  [SvgText value:perceptronX4 textAnchor:"start" fontFamily:"Helvetica, Arial, sans-serif" fontSize:"47" y:"265.76917" x:"470" strokeOpacity:"null" strokeWidth:"0" stroke:"#000" fill:"#000000" /]
  [SvgText value:"activation" textAnchor:"start" fontFamily:"Helvetica, Arial, sans-serif" fontSize:"30" y:"240" x:"600" strokeOpacity:"null" strokeWidth:"0" stroke:"#000" fill:"#000000" /]
  [SvgText value:"(tanh)" textAnchor:"start" fontFamily:"Helvetica, Arial, sans-serif" fontSize:"30" y:"280" x:"620" strokeOpacity:"null" strokeWidth:"0" stroke:"#000" fill:"#000000" /]
  [SvgText value:perceptronY textAnchor:"start" fontFamily:"Helvetica, Arial, sans-serif" fontSize:"47" y:"265" x:"820" strokeOpacity:"null" strokeWidth:"0" stroke:"#000" fill:"#000000" /]
  [SvgText value:"y" textAnchor:"start" fontFamily:"Helvetica, Arial, sans-serif" fontSize:"47" y:"260" x:"940" strokeOpacity:"null" strokeWidth:"0" stroke:"#000" fill:"#000000" /]
[/SVG]
X1: [customRange className:"slider" style:`{width: '22%'}` value:perceptronX1 min:0 max:3 step:1/]
X2: [customRange className:"slider" style:`{width: '22%'}` value:perceptronX2 min:0 max:3 step:1 /]
X3: [customRange className:"slider" style:`{width: '22%'}` value:perceptronX3 min:0 max:3 step:1 /]
W1: [customRange className:"slider" style:`{width: '22%'}` value:perceptronW1 min:0 max:1 step:0.1 /]
W2: [customRange className:"slider" style:`{width: '22%'}` value:perceptronW2 min:0 max:1 step:0.1 /]
W3: [customRange className:"slider" style:`{width: '22%'}` value:perceptronW3 min:0 max:1 step:0.1 /]

The example from above can be written in one equation:

[equation display:true] y = tanh(x_1*w_1+x_2*w_2+x_3*w_3) [/equation]

where [equation] y [/equation] is the prediction. But what is the purpose of the activation function?
It adds non-linearity, and thus, makes complex behavior possible in the first place. Why?
Because the result of chaining two linear perceptrons together is still linear, thus, does not provide a better prediction:

[equation display:true] y_1 = x*w_1 [/equation]
[equation display:true latex:"y_2 = y_1*w_2 = x*\overbrace{w_1*w_2}^{w}" /]

It is important to keep in mind that that the input [equation] x [/equation] represents anything a computer can measure e.g. a pixel.
Generally, a pixel can have a value between 0 and 255 where zero means that the pixel is turned off.
By mixing red, green, and blue pixels, nearly every color can be created e.g. (red=255, green=255, blue=0) results in yellow.
A (digital) image is simply a composition of millions of pixels.

[var name:"pixelValue" value:0.5 /]
[div className:"centered"]
  [div style:`{opacity: 1 - pixelValue, width: 75, height: 75, background: 'black', margin: '20px auto'}` /]
  [div]Value: [equation latex:`Number(pixelValue*255).toFixed(0)` /][/div] 
  [customRange className:"slider" value:pixelValue min:0 max:1 step:0.01 /]
[/div]

## Learning Task
Create a perceptron with red, green, and blue pixels as inputs and connect them to an output node.
Try different input values, weights, and activation functions and think about the effect they have on the flow of information.

![example](static/images/task_perceptron.gif)

## Hints
- What is the equation of the perceptron?
- Use the equation to calculate the prediction value of e.g. red=255, green=120, blue=0, weights=0.5, a(x)=x. What is the interpretation?
- Change the weight of the red pixel to 1 and all other weights to 0. What is the interpretation?
- Use the activation function [equation] a(x)=tanh(x) [/equation]. What is the value of the prediction? How does the interpretation change? When is [equation] tanh(x) [/equation] -1 or 1?
- Another popular activation function is called "sigmoid" ([equation] a(x)=1/(1+e^{-x}) [/equation]). Draw the sigmoid function as well as [equation] a(x)=x [/equation] and [equation] a(x)=tanh(x) [/equation]. What range can they take?

[/Step]
[Step]

## Loss Function

Identifying and naming objects in an image is no challenge for humans.
Every child can identify a dog and, if not, learn it in an instant e.g. by showing a dog nearby.
To translate this "learning by example" to a computer, it is necessary to quantify (express as numbers) this process.
Just like grades in school, the computer has to know how good it is.
For example, when a computer identifies a car instead of a dog, the grade should be low. 
When it mistakes a Yorkshire Terrier for a West Highland Terrier, it should get a comparatively high grade.
In machine learning terms, this is called a loss function.
As opposed to school grades - the smaller the loss, the better:

[equation display:true] loss = \frac{1}{2}(t-y)^2 [/equation]

where [equation] t [/equation] is the true label a human provides, and [equation] y [/equation] is the prediction of the computer.
Why is it multiplied by [equation] 1/2 [/equation] and squared?
Because [equation] 1/2 [/equation] simplifies the derivative (next section), and the square creates a lower boundary for the loss (deep learning minimizes the loss, next section).
There is no particular reason to choose this exact loss function (e.g. [equation] |t-y| [/equation] works as well), but the mathematical properties make it easier to work with.

[var name:"lossAnimal" value:"static/images/dog_2.svg" /]
[derived name:'lossAnimalValue' value:`lossAnimal=="static/images/dog_2.svg" ? 1 : (lossAnimal=="static/images/cat.svg" ? 4 : 8)` /]
[svg viewBox:"0 30 800 250"]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" x:"57.09375" y:"199.5" fontSize:"43" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" opacity:"0.8"]loss =[/text]
  [svgPath fill:"#fff" strokeWidth:"6.5" strokeOpacity:"null" fillOpacity:"null" opacity:"0.5" d:"m298.5,117.5" stroke:"#000"/]
  [svgPath fill:"#fff" strokeWidth:"6.5" strokeOpacity:"null" fillOpacity:"null" opacity:"0.8" d:"m229.5,120.5c-51,70.05745 0,133.00763 -0.5,132.49994" stroke:"#000"/]
  [svgPath fill:"#fff" strokeWidth:"6.5" strokeOpacity:"null" fillOpacity:"null" opacity:"0.8" d:"m579.5,122.00021c57.27527,58 9.32385,130.00003 8.6579,129.5" stroke:"#000"/]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" x:"607.5" y:"126.5" fontSize:"43" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" opacity:"0.8"]2[/text]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" x:"652.5" y:"198.5" fontSize:"43" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" opacity:"0.8"]=[/text]
  [svgText value:lossAnimalValue fill:"#000000" stroke:"#000" strokeWidth:"0" x:"702.09375" y:"200.5" fontSize:"43" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" opacity:"0.8" /]
  [text fill:"#000000" strokeWidth:"0" x:"360.89112" y:"217.5" fontSize:"101" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" opacity:"0.8" transform:"matrix(1.45748 0 0 1 -145.442 0)" stroke:"#000"]-[/text]
  [image x:"257.5" y:"111.5" width:"121" height:"164" href:"static/images/dog_1.svg" /]
  [image x:"452.5" y:"122.5" width:"101" height:"134" href:lossAnimal /]
  [svgPath stroke:"#000" d:"m259.5,61.5c42.80383,17.18239 52.65793,45.8197 52.50397,45.50151" opacity:"0.3" strokeWidth:"6.5" fill:"#fff"/]
  [line stroke:"#000" opacity:"0.3" strokeLinecap:"null" strokeLinejoin:"null" y2:"58.5" x2:"275.5" y1:"61.5" x1:"259.5" fillOpacity:"null" strokeOpacity:"null" strokeWidth:"6.5" fill:"none"/]
  [line stroke:"#000" opacity:"0.3" strokeLinecap:"null" strokeLinejoin:"null" y2:"72.5" x2:"258.5" y1:"59.5" x1:"261.5" fillOpacity:"null" strokeOpacity:"null" strokeWidth:"6.5" fill:"none"/]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" x:"236.09375" y:"65.5" fontSize:"43" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" opacity:"0.3"]t[/text]
  [svgPath stroke:"#000" d:"m538.5,63.5c-32.19659,17.18239 -39.60873,45.8197 -39.49292,45.50151" opacity:"0.3" strokeWidth:"6.5" fill:"#fff"/]
  [line stroke:"#000" opacity:"0.3" strokeLinecap:"null" strokeLinejoin:"null" y2:"65.5" x2:"538.5" y1:"56.5" x1:"524.5" fillOpacity:"null" strokeOpacity:"null" strokeWidth:"6.5" fill:"none"/]
  [line stroke:"#000" opacity:"0.3" strokeLinecap:"null" strokeLinejoin:"null" y2:"78.5" x2:"533.5" y1:"61.5" x1:"537.5" fillOpacity:"null" strokeOpacity:"null" strokeWidth:"6.5" fill:"none"/]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" x:"548.09375" y:"61.5" fontSize:"43" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" opacity:"0.3"]y[/text]
[/svg]
[div className:"centered" style:`{marginBottom:'10px'}`]
  [customRadio value:lossAnimal options:`[{ label: "Dog \u00a0", value: "static/images/dog_2.svg"}, { label: "Cat \u00a0", value: "static/images/cat.svg" }, { label: "Tea \u00a0", value: "static/images/tea.svg" }]` /]
[/div]

As mentioned in the previous section, an image consists of millions of pixels.
Hence, the loss has to be calculated (and summed together) for each of those pixels.
For example, a loss function with one pixel might look like this:

[var name:lossPixel1 value:180 /]
[var name:lossPixel2 value:100 /]
[derived name:lossPixelResult value:`(lossPixel1-lossPixel2)**2` /]
[svg viewBox:"0 30 1100 300"]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" x:"57.09375" y:"199.5" fontSize:"43" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" opacity:"0.8"]loss =[/text]
  [svgPath fill:"#fff" strokeWidth:"6.5" strokeOpacity:"null" fillOpacity:"null" opacity:"0.5" d:"m298.5,117.5" stroke:"#000"/]
  [svgPath fill:"#fff" strokeWidth:"6.5" strokeOpacity:"null" fillOpacity:"null" opacity:"0.8" d:"m229.5,120.5c-51,70.05745 0,133.00763 -0.5,132.49994" stroke:"#000"/]
  [svgPath fill:"#fff" strokeWidth:"6.5" strokeOpacity:"null" fillOpacity:"null" opacity:"0.8" d:"m579.5,122.00021c57.27527,58 9.32385,130.00003 8.6579,129.5" stroke:"#000"/]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" x:"607.5" y:"126.5" fontSize:"43" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" opacity:"0.8"]2[/text]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" x:"652.5" y:"198.5" fontSize:"43" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" opacity:"0.8"]=[/text]
  [svgText fill:"#000000" stroke:"#000" strokeWidth:"0" x:"702.09375" y:"200.5" fontSize:"43" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" opacity:"0.8"]
    [svgTSpan value:"(" /]
    [svgTSpan value:lossPixel1 /]
    [svgTSpan value:"-" /]
    [svgTSpan value:lossPixel2 /]
    [svgTSpan value:")" /]
    [svgTSpan value:"2" dy:"-10" /]
    [svgTSpan value:" = " dy:"10" /]
    [svgTSpan value:lossPixelResult /]
  [/svgText]
  [text fill:"#000000" strokeWidth:"0" x:"360.89112" y:"217.5" fontSize:"101" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" opacity:"0.8" transform:"matrix(1.45748 0 0 1 -145.442 0)" stroke:"#000"]-[/text]
  [rect fill:`"rgb(" + lossPixel1 + "," + lossPixel1 + "," + lossPixel1 + ")"` stroke:"#000" strokeWidth:"1.5" x:"260" y:"140" width:"100" height:"100" /]
  [rect fill:`"rgb(" + lossPixel2 + "," + lossPixel2 + "," + lossPixel2 + ")"` stroke:"#000" strokeWidth:"1.5" x:"450" y:"140" width:"100" height:"100" /]
  [svgPath stroke:"#000" d:"m259.5,61.5c42.80383,17.18239 52.65793,45.8197 52.50397,45.50151" opacity:"0.3" strokeWidth:"6.5" fill:"#fff"/]
  [line stroke:"#000" opacity:"0.3" strokeLinecap:"null" strokeLinejoin:"null" y2:"58.5" x2:"275.5" y1:"61.5" x1:"259.5" fillOpacity:"null" strokeOpacity:"null" strokeWidth:"6.5" fill:"none"/]
  [line stroke:"#000" opacity:"0.3" strokeLinecap:"null" strokeLinejoin:"null" y2:"72.5" x2:"258.5" y1:"59.5" x1:"261.5" fillOpacity:"null" strokeOpacity:"null" strokeWidth:"6.5" fill:"none"/]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" x:"236.09375" y:"65.5" fontSize:"43" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" opacity:"0.3"]t[/text]
  [svgPath stroke:"#000" d:"m538.5,63.5c-32.19659,17.18239 -39.60873,45.8197 -39.49292,45.50151" opacity:"0.3" strokeWidth:"6.5" fill:"#fff"/]
  [line stroke:"#000" opacity:"0.3" strokeLinecap:"null" strokeLinejoin:"null" y2:"65.5" x2:"538.5" y1:"56.5" x1:"524.5" fillOpacity:"null" strokeOpacity:"null" strokeWidth:"6.5" fill:"none"/]
  [line stroke:"#000" opacity:"0.3" strokeLinecap:"null" strokeLinejoin:"null" y2:"78.5" x2:"533.5" y1:"61.5" x1:"537.5" fillOpacity:"null" strokeOpacity:"null" strokeWidth:"6.5" fill:"none"/]
  [text fill:"#000000" stroke:"#000" strokeWidth:"0" x:"548.09375" y:"61.5" fontSize:"43" fontFamily:"Helvetica, Arial, sans-serif" textAnchor:"start" opacity:"0.3"]y[/text]
[/svg]
True: [customRange className:"slider" value:lossPixel1 min:0 max:255 /]
Predicted: [customRange className:"slider" value:lossPixel2 min:0 max:255 /]

## Learning Task
Use the perceptron from the previous task.
Try different true values [equation] t [/equation] and think about how they affect the loss.
Moreover, think about how the input, weights, and activation function change the loss.

![example](static/images/task_loss_function.gif)

## Hints
- Because of the range of [equation] tanh(x) [/equation] (between -1 and 1), it is better to use inputs with a similar range. For example, a pixel can be represented by values between zero and one where 1 means completely turned on).
- When is it possible for a loss to never approach zero? Try different true values and activation functions.

[/Step]
[Step]

## Gradient Descent

Just like a student, the computer should try to improve its' grades (the actual "learning"). 
In other words, it should try to minimize the loss function, because, the smaller the loss, the better the prediction.
But how to minimize a function? The most straightforward answer is by taking the derivative and set it equal to zero [equation] loss' = 0 [/equation].
However, this analytical approach does not work well for complex functions with millions of weights as used in deep learning.
For this reason, an iterative approach called "gradient descent" is used. The word "gradient" is just another word for "derivative" (with multiple variables).
Instead of setting the derivative equal to zero, the weights are adjusted step-by-step by going in the opposite direction of the derivative.
As you may have guessed, "learning" is nothing more than adjusting the weights [equation] w [/equation] such that the loss gets smaller (better grades).

[equation display:true] w_{new} = w_{old} - LearningRate*loss' [/equation]

For example, given [equation] loss = w^2 [/equation] with derivative [equation] loss' = 2w [/equation], we have to choose a starting weight (e.g. [equation] w=1 [/equation]) and a learning rate (e.g. [equation] LearningRate=0.8 [/equation]).
In the first step, the weight is adjusted to [equation] w_{new} = 1 - 0.8*(2*1) = -0.6 [/equation], in the second step to [equation] w_{new} = -0.6 - 0.8*(2*-0.6) = 0.36 [/equation] etc.
As you see, the weight gradually approaches zero because this is where the loss is the smallest.
Similar to a ball rolling off a mountain, the weights change the fastest where the slope is the steepest.
Keep in mind that the learning rate has to be large enough to make a difference each step but not too large because it can overshoot.
A visualization with adjustable learning rate (e.g. 0.1 is relatively slow and 1.1 overshoots) can be found below.

[var name:"gradientVisualizationWeights" value:`[1.0]` /]
[var name:"gradientVisualizationLearningRate" value:0.8 /]
[var name:"gradientVisualizationShowCurve" value:false /]
[gradientVisualization weights:gradientVisualizationWeights showCurve:gradientVisualizationShowCurve /]
[div className:"centered"]
  [customRange className:"slider" value:gradientVisualizationLearningRate min:0 max:1.1 step:0.1/]
[/div]
[div className:"centered"]
  Learning Rate: [Display value:gradientVisualizationLearningRate /] 
  [button className:"button" onClick:`gradientVisualizationWeights.push(gradientVisualizationWeights[gradientVisualizationWeights.length - 1] - gradientVisualizationLearningRate*2*gradientVisualizationWeights[gradientVisualizationWeights.length - 1]);`]Train[/button]
  [button className:"button" onClick:`gradientVisualizationWeights = [1.0]; gradientVisualizationLearningRate=0.8`]Reset[/button]
  Show Line: [Boolean value:gradientVisualizationShowCurve /]
[/div]

## Learning Task
Use the perceptron from the previous task and visualize the gradients.
Train the perceptron, use different learning rates and think about how the gradient is used to adjust the weight.

![example](static/images/task_gradient_descent.gif)

## Hints
- What happens when the learning rate is either too small or too large?
- What happens to the gradient when [equation] t [/equation] changes? Why is the gradient zero when [equation] t=y [/equation]?
- What happens to the gradient when [equation] x [/equation] changes? Why is the gradient zero when [equation] x=0 [/equation]?
- What happens to the gradient when [equation] a(x) [/equation] changes? Why is the gradient zero with [equation] a(x)=tanh(x) [/equation] and a large input (e.g. 255)?
- Choose one specific weight and predict how it will change when you train the perceptron. [equation] w_{new} = w_{old} - LearningRate*loss' [/equation]

[/Step]
[Step]

## Backpropagation Theory

Gradient descent uses the derivative to minimize the loss function. 
But how to calculate the derivative of multiple chained perceptrons (layers)? This is where backpropagation is used. 
Mathematically, backpropagation is simply the repeated application of the chain rule:

[equation display:true] f'(x) = g'(h(x)) * h'(x) [/equation]

For example, given the following network:

[img src:"static/images/task_backpropagation.png" /]
[equation display:true] n = sin((x_0*w_0)+(x_1*w_1)) [/equation]
[equation display:true] y = sin(n*w_2) [/equation]

The goal is to calculate the derivative of each weight (how much the loss changes when the weight changes).
Due to the chain rule, it makes sense to start at the outermost weight and continue till reaching the innermost weight.
First, we have to calculate the derivative with respect to [equation] y [/equation].
This means that everything except [equation] y [/equation] is treated as a constant (e.g. insert numerical values such as [equation] t=5 [/equation]).

[equation display:true] loss = \frac{1}{2}(t-y)^2 [/equation]
[equation display:true] loss_y' = (t-y)*-y' = (t-y)*-cos(n*w_2) [/equation]

Now we can calculate the derivative with respect to [equation] w_2 [/equation].
Since we already have [equation] loss_y' [/equation], there is no need to start from scratch.
We can simply continue at the derivative of [equation] n*w_2 [/equation], and because everything except [equation] w_2 [/equation] is treated as a constant, the derivative is just [equation] n [/equation].

[equation display:true] loss_{w_2}' = loss_y'*n [/equation]

The next step is to calculate the derivative with respect to [equation] n [/equation].
Again, we can build on the previous results.

[equation display:true] loss_n' = loss_y'*cos((x0*w0)+(x1*w1))*w_2 [/equation]

Finally, we can calculate the derivative with respect to [equation] w_0 [/equation] and [equation] w_1 [/equation].

[equation display:true] loss_{x_0} = loss_n'*x_0 [/equation]
[equation display:true] loss_{x_1}' = loss_n'*x_1 [/equation]

As we can see, the derivatives build on each other which is why it is called backpropagation (backwards propagation of the errors).
In a way, backpropagation is similar to a group project with dependent tasks.
The first student has to finish his/her task and passes it on to the next student (dividing the learning).

## Learning Task
Try different network topologies and see how they change the flow of the gradient.

![example](static/images/task_backpropagation_theory.gif)

## Hints
For real-world deep learning projects, an understanding of the gradients is essential, especially for debugging.
When the gradient is too small or too large, the network will not be able to learn.

- What happens to the gradient when multiple perceptrons with [equation] tanh(x) [/equation] activation function are chained together? (Keyword: vanishing gradient)
- What happens to the gradient when a weight is extremely large? Why can it break the learning of a network? (Keyword: random weights)

[/Step]
[Step]

## Backpropagation Implementation

Calculating derivatives by hand is cumbersome and error-prone.
For this reason, it is a good idea to automate this process.
The algorithm is usually divided into forwardpropagation and backpropagation.
The forwardpropagation starts from the inputs, calculates the value of the perceptron, and repeats this till every perceptron has a value.
The backpropagation builds on the forwardpropagation results and calculates the derivatives of each weight.

For example, a simple recursive forwardpropagation function can be found below.
In general, the forwardpropagation algorithm is easy to implement (iteratively or recursively) because it does not rely on higher mathematics.
Note that the provided code-examples do not save results nor apply optimization techniques (for simplicity reasons).

```javascript
function forwardpropagation(graph, node, results) {
  // get all ingoing edges from node
  let edges = graph.filter(e => e.target == node.id);
  if (edges.length > 0) {
    let sum = 0;
    for (let j in edges) {
      let edge = edges[j];
      let nextNode = graph.filter(e => e.id == edge.source)[0];
      // recursive call
      let nextNodeValue = forwardpropagation(graph, nextNode, results);
      // calculate weighted sum of perceptron e.g. x1*w1+...+xn*wn
      sum += edge.data.value * nextNodeValue;
    }
    // result --> apply activation function e.g. a(x1*w1+...+xn*wn)
    let value = math.evaluate(node.data.activation.replace('x', sum));
    return value;
  } else if (node.id.includes("x")) {
    // stop recursion when reaching an input node
    return node.data.value;
  } else {
    return NaN;
  }
}
```

Backpropagation is a bit trickier to implement.
Because of the recursive nature of the chain rule, it makes sense to implement backpropagation recursively but this is completely up to you.
An example implementation can be found below.

```javascript
function backpropagation(graph, node, forwardresults, results) {
  // get all outgoing edges from node
  let edges = graph.filter(element => element.source == node.id);
  if (edges.length > 0) {
    let totalGradient = 0.0;
    for (let j in edges) {
      let edge = edges[j];
      let nextNode = graph.filter(element => element.id == edge.target)[0];
      // recursive call
      let nextNodeGradient = backpropagation(graph, nextNode, forwardresults, results);
      // calculate gradient
      let outerDerivative = math.derivative(node.data.activation, 'x');
      totalGradient += nextNodeGradient * outerDerivative.evaluate({ x: node.data.sum }) * edge.data.value;
      // result --> the gradient of the weight
      let edgeGradient = nextNodeGradient * node.data.value;
    }
    // result --> the total gradient is returned (propagated backwards)
    return totalGradient;
  } else if (node.id.includes("y")) {
    // stop recursion when reaching an output node
    let outerDerivative = math.derivative(node.data.activation, 'x');
    return  -1 * (node.data.true - node.data.value) * outerDerivative.evaluate({ x: node.data.sum });
  } else {
    return NaN;
  }
}
```

## Learning Task
Implement your own version of backpropagation because it is the best way to deepen your understanding.

![example](static/images/task_backpropagation_implementation.gif)

## Hints
- Feel free to have a look at the provided implementations. "Complete" supports symbolic equations whereas "Minimal" is kept as simple as possible.
- Remember the theory! It is almost a 1:1 implementation of the theory.

[equation display:true] loss_y' = (t-y)*-y' [/equation]

```javascript
let outerDerivative = math.derivative(node.data.activation, 'x');
return  -1 * (node.data.true - node.data.value) * outerDerivative.evaluate({ x: node.data.sum });
```

[equation display:true] loss_{w_2}' = loss_y'*n [/equation]

```javascript
let edgeGradient = nextNodeGradient * node.data.value;
```


[/Step]

[/flowChart]
[/FullWidth]